{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_last_3_P</th>\n",
       "      <th>home_last_3_OppP</th>\n",
       "      <th>home_last_3_FG</th>\n",
       "      <th>home_last_3_FGA</th>\n",
       "      <th>home_last_3_FG%</th>\n",
       "      <th>home_last_3_3P</th>\n",
       "      <th>home_last_3_3PA</th>\n",
       "      <th>home_last_3_3P%</th>\n",
       "      <th>home_last_3_FT</th>\n",
       "      <th>home_last_3_FTA</th>\n",
       "      <th>...</th>\n",
       "      <th>away_last_50_OeFG%</th>\n",
       "      <th>away_last_50_OTOV%</th>\n",
       "      <th>away_last_50_OORB%</th>\n",
       "      <th>away_last_50_OFT/FGA</th>\n",
       "      <th>away_last_50_DeFG%</th>\n",
       "      <th>away_last_50_DTOV%</th>\n",
       "      <th>away_last_50_DDRB%</th>\n",
       "      <th>away_last_50_DFT/FGA</th>\n",
       "      <th>away_last_50_W</th>\n",
       "      <th>away_last_50_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.33</td>\n",
       "      <td>106.33</td>\n",
       "      <td>42.33</td>\n",
       "      <td>84.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.33</td>\n",
       "      <td>31.33</td>\n",
       "      <td>0.36</td>\n",
       "      <td>16.33</td>\n",
       "      <td>22.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.55</td>\n",
       "      <td>22.63</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.51</td>\n",
       "      <td>12.67</td>\n",
       "      <td>76.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>41.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.67</td>\n",
       "      <td>114.33</td>\n",
       "      <td>41.33</td>\n",
       "      <td>90.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>11.33</td>\n",
       "      <td>31.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>16.67</td>\n",
       "      <td>19.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.71</td>\n",
       "      <td>25.02</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.26</td>\n",
       "      <td>77.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>113.67</td>\n",
       "      <td>113.33</td>\n",
       "      <td>44.00</td>\n",
       "      <td>92.67</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.33</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>15.33</td>\n",
       "      <td>17.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.52</td>\n",
       "      <td>10.79</td>\n",
       "      <td>24.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>11.25</td>\n",
       "      <td>75.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116.33</td>\n",
       "      <td>117.67</td>\n",
       "      <td>44.33</td>\n",
       "      <td>95.33</td>\n",
       "      <td>0.47</td>\n",
       "      <td>14.33</td>\n",
       "      <td>38.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>13.33</td>\n",
       "      <td>15.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54</td>\n",
       "      <td>12.56</td>\n",
       "      <td>23.80</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.99</td>\n",
       "      <td>76.40</td>\n",
       "      <td>0.16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.67</td>\n",
       "      <td>112.33</td>\n",
       "      <td>43.00</td>\n",
       "      <td>89.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>13.33</td>\n",
       "      <td>31.33</td>\n",
       "      <td>0.42</td>\n",
       "      <td>21.33</td>\n",
       "      <td>27.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.51</td>\n",
       "      <td>10.69</td>\n",
       "      <td>27.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.54</td>\n",
       "      <td>13.79</td>\n",
       "      <td>74.56</td>\n",
       "      <td>0.21</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   home_last_3_P  home_last_3_OppP  home_last_3_FG  home_last_3_FGA  \\\n",
       "0         112.33            106.33           42.33            84.67   \n",
       "1         110.67            114.33           41.33            90.67   \n",
       "2         113.67            113.33           44.00            92.67   \n",
       "3         116.33            117.67           44.33            95.33   \n",
       "4         120.67            112.33           43.00            89.00   \n",
       "\n",
       "   home_last_3_FG%  home_last_3_3P  home_last_3_3PA  home_last_3_3P%  \\\n",
       "0             0.50           11.33            31.33             0.36   \n",
       "1             0.46           11.33            31.33             0.34   \n",
       "2             0.48           10.33            29.00             0.34   \n",
       "3             0.47           14.33            38.00             0.39   \n",
       "4             0.49           13.33            31.33             0.42   \n",
       "\n",
       "   home_last_3_FT  home_last_3_FTA  ...  away_last_50_OeFG%  \\\n",
       "0           16.33            22.67  ...                0.54   \n",
       "1           16.67            19.67  ...                0.52   \n",
       "2           15.33            17.67  ...                0.52   \n",
       "3           13.33            15.33  ...                0.54   \n",
       "4           21.33            27.33  ...                0.51   \n",
       "\n",
       "   away_last_50_OTOV%  away_last_50_OORB%  away_last_50_OFT/FGA  \\\n",
       "0               11.55               22.63                  0.18   \n",
       "1               11.71               25.02                  0.18   \n",
       "2               10.79               24.07                  0.15   \n",
       "3               12.56               23.80                  0.19   \n",
       "4               10.69               27.32                  0.18   \n",
       "\n",
       "   away_last_50_DeFG%  away_last_50_DTOV%  away_last_50_DDRB%  \\\n",
       "0                0.51               12.67               76.90   \n",
       "1                0.53               11.26               77.10   \n",
       "2                0.53               11.25               75.18   \n",
       "3                0.50               11.99               76.40   \n",
       "4                0.54               13.79               74.56   \n",
       "\n",
       "   away_last_50_DFT/FGA  away_last_50_W  away_last_50_H  \n",
       "0                  0.18            41.0            27.0  \n",
       "1                  0.21            18.0            26.0  \n",
       "2                  0.17            19.0            26.0  \n",
       "3                  0.16            32.0            25.0  \n",
       "4                  0.21            27.0            24.0  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import dataset\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.head()\n",
    "\n",
    "\n",
    "#We want clean the data and split up the input and output\n",
    "target_column = \"Home_Team_Won\"\n",
    "unneeded_columns = [\"Date\", \"Home_Team\", \"Away_Team\"]\n",
    "\n",
    "\n",
    "#remove first column from dataset\n",
    "df = df.drop(df.columns[[0]],axis = 1)\n",
    "\n",
    "\n",
    "#remove unneed_columns. These are columns that are in our dataset, but we don't want to feed into the model\n",
    "#this is because they are categorical, we're only feeding our model numerical data\n",
    "clean_df = df.drop(columns=unneeded_columns, axis=1)\n",
    "\n",
    "\n",
    "# making new data frame with dropped NA values\n",
    "clean_df = clean_df.dropna(axis=0, how='any')\n",
    "\n",
    "\n",
    "#get the output rows of the dataset\n",
    "output_rows = clean_df[target_column]\n",
    "\n",
    "\n",
    "#remove output rows from the dataset\n",
    "clean_df = clean_df.drop(columns=target_column, axis=1)\n",
    "\n",
    "\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1274, 228)\n",
      "y_train: (1274,)\n",
      "X_test: (425, 228)\n",
      "y_test: (425,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(clean_df)\n",
    "clean_df = scaler.transform(clean_df)\n",
    "#we cleaned up our data, now we are going to split it and get it ready to feed into the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, output_rows, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "#now we have our training and testing dataset\n",
    "print('X_train:',np.shape(X_train))\n",
    "print('y_train:',np.shape(y_train))\n",
    "print('X_test:',np.shape(X_test))\n",
    "print('y_test:',np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer and output layer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units = 228, activation = 'relu', input_shape = (228, )))\n",
    "model.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5832\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.6020\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.6185\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.6499\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6469 - accuracy: 0.6279\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.6531\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.6358\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6318 - accuracy: 0.6444\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.6460\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.6413\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.6554\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.6476\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.6531\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6341 - accuracy: 0.6201\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6274 - accuracy: 0.6531\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.6507\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6203 - accuracy: 0.6617\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6396 - accuracy: 0.6460\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6247 - accuracy: 0.6358\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.6570\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6057 - accuracy: 0.6711\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.6735\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6625\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.6648\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6142 - accuracy: 0.6609\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.6586\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6008 - accuracy: 0.6805\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6625\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.6766\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.6025 - accuracy: 0.6648\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.6035 - accuracy: 0.6688\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.6892\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5935 - accuracy: 0.6821\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.6797\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.6954\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.6845\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5765 - accuracy: 0.6939\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5918 - accuracy: 0.6845\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.6923\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.6892\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5741 - accuracy: 0.7057\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.7017\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.6986\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.6978\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.6868\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.6986\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7111\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.6986\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.7119\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7166\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7347\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7276\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.7253\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5643 - accuracy: 0.7041\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7378\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7441\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7347\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7355\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7457\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.7449\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7449\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7535\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7433\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.7630\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7810\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7771\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7873\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7786\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7865\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7755\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4608 - accuracy: 0.7841\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7849\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4693 - accuracy: 0.7724\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7857\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7865\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7936\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4493 - accuracy: 0.7943\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.8148\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.8085\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.8069\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7928\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8093\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.8030\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.8124\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4050 - accuracy: 0.8289\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8257\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8289\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.8069\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4114 - accuracy: 0.8171\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8265\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8407\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.4026 - accuracy: 0.8234\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3861 - accuracy: 0.8305\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8462\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3699 - accuracy: 0.8493\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8579\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8422\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8336\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8626\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207f974fa10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\backend.py\", line 5707, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 4) vs (None, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_and_metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(X_test, y_test)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(loss_and_metrics)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoss = \u001b[39m\u001b[39m'\u001b[39m,loss_and_metrics[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefy4jv8fa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\nbadh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\backend.py\", line 5707, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 4) vs (None, 1)).\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test)\n",
    "print(loss_and_metrics)\n",
    "print('Loss = ',loss_and_metrics[0])\n",
    "print('Accuracy = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
